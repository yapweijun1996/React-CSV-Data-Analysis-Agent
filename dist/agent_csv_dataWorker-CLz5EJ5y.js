const _=r=>{if(r==null)return null;let e=String(r).trim();if(e==="")return null;let t=!1;e.startsWith("(")&&e.endsWith(")")&&(e=e.substring(1,e.length-1),t=!0),e=e.replace(/[$\s€£¥%]/g,"");const o=e.lastIndexOf(","),s=e.lastIndexOf(".");o>s?e=e.replace(/\./g,"").replace(",","."):e=e.replace(/,/g,"");const n=parseFloat(e);return isNaN(n)?null:t?-n:n},A=r=>{if(!r||r.length===0)return[];const e=Object.keys(r[0]),t=[];for(const o of e){let s=!0;const n=r.map(i=>i[o]);let c=0;for(const i of n){const d=_(i);if(i!==null&&String(i).trim()!==""){if(d===null){s=!1;break}c++}}if(s&&c>0){const i=n.map(_).filter(d=>d!==null);t.push({name:o,type:"numerical",valueRange:[Math.min(...i),Math.max(...i)],missingPercentage:(1-i.length/r.length)*100})}else{const i=new Set(n.map(String));t.push({name:o,type:"categorical",uniqueValues:i.size,missingPercentage:n.filter(d=>d===null||String(d).trim()==="").length/r.length*100})}}return t};let I=null;const y=()=>typeof indexedDB<"u"&&"IDBDatabase"in self,S=async()=>{if(!y())throw new Error("IndexedDB is not available inside the worker context.");return I||(I=import("./agent_csv_csvAgentDb-DhthqrSh.js")),I},F=2e3,M=1e3,R=3e3,x=3e3,v=r=>{let e=0;for(let t=0;t<r.length;t++)e=(e<<5)-e+r.charCodeAt(t),e|=0;return Math.abs(e).toString(36)},b=r=>r==null?"":typeof r=="string"?r.trim():String(r),P=(r,e,t=3)=>{const o=[];for(const s of r){const n=s[e];if(n==null||n==="")continue;const c=b(n);if(c&&(o.includes(c)||o.push(c),o.length>=t))break}return o},T=async r=>{const{datasetId:e,sampleSize:t=F}=r;if(!e)throw new Error("datasetId is required for profiling.");const o=await S(),[s,n]=await Promise.all([o.readColumnStoreRecord(e),o.readSampledRows(e,t)]);if(!s)throw new Error("No cached metadata for this dataset. Try re-uploading the CSV.");if(n.length===0)throw new Error("No rows available to profile.");const i=A(n).map(w=>({name:w.name,type:w.type,distinct:w.uniqueValues??new Set(n.map(E=>E[w.name])).size,emptyPercentage:w.missingPercentage??0,examples:P(n,w.name)})),d=[];return s.rowCount>n.length&&d.push(`Profiled ${n.length.toLocaleString()} rows (sample) out of ${s.rowCount.toLocaleString()}.`),{rowCount:s.rowCount,sampledRows:n.length,columns:i,warnings:d}},D=(r,e)=>!e||e.length===0?!0:e.every(t=>{const o=r[t.column],s=b(o),n=b(t.value??"");switch(t.op){case"eq":return t.caseInsensitive?s.toLowerCase()===n.toLowerCase():s===n;case"neq":return t.caseInsensitive?s.toLowerCase()!==n.toLowerCase():s!==n;case"gt":return Number(s)>Number(n);case"gte":return Number(s)>=Number(n);case"lt":return Number(s)<Number(n);case"lte":return Number(s)<=Number(n);case"contains":return s.toLowerCase().includes(n.toLowerCase());case"in":return Array.isArray(t.values)?t.values.some(c=>b(c)===s):!1;default:return!0}}),O=r=>{if(r==null)return null;if(typeof r=="number"&&Number.isFinite(r))return r;const e=b(r).replace(/[$,%\s]/g,"").replace(/,/g,"");if(!e)return null;const t=Number(e);return Number.isFinite(t)?t:null},k=(r,e,t)=>{var L;const o=e.mode??"sample";if(!Array.isArray(e.metrics)||e.metrics.length===0)throw new Error("metrics[] is required for aggregation.");if(o==="full"&&!e.allowFullScan)throw new Error("FULL_SCAN_BLOCKED");const s=e.by??[],n=new Map,c=((L=e.filter)==null?void 0:L.length)??0,i=performance.now()+x;for(const m of r){if(!D(m,e.filter))continue;if(performance.now()>i){if(o==="full")throw new Error("AGG_TIMEOUT");break}const g=s.map(a=>b(m[a])),f=g.length>0?g.join("||"):"__total__";if(!n.has(f)){const a={};s.forEach((u,p)=>{a[u]=g[p]});const l={};e.metrics.forEach(u=>{const p=u.as??`${u.fn}_${u.column??"rows"}`;l[p]={sum:0,count:0,min:Number.POSITIVE_INFINITY,max:Number.NEGATIVE_INFINITY}}),n.set(f,{keys:a,accumulators:l})}const h=n.get(f);e.metrics.forEach(a=>{const l=a.as??`${a.fn}_${a.column??"rows"}`,u=h.accumulators[l];if(a.fn==="count"){u.count+=1,u.sum+=1;return}if(!a.column)throw new Error(`Metric ${a.fn} requires a column.`);const p=O(m[a.column]);p!==null&&(u.count+=1,u.sum+=p,u.min=Math.min(u.min,p),u.max=Math.max(u.max,p))})}const d=[...s.map(m=>({name:m,type:"string"})),...e.metrics.map(m=>({name:m.as??`${m.fn}_${m.column??"rows"}`,type:"number"}))],w=[];for(const m of n.values()){const g={...m.keys};e.metrics.forEach(f=>{const h=f.as??`${f.fn}_${f.column??"rows"}`,a=m.accumulators[h];let l=null;switch(f.fn){case"sum":l=a.sum;break;case"avg":l=a.count>0?a.sum/a.count:null;break;case"count":l=a.count;break;case"min":l=a.min===Number.POSITIVE_INFINITY?null:a.min;break;case"max":l=a.max===Number.NEGATIVE_INFINITY?null:a.max;break}g[h]=l??0}),w.push(g)}e.orderBy&&e.orderBy.length>0&&w.sort((m,g)=>{for(const f of e.orderBy){const h=(f.direction??"desc").toLowerCase()==="asc"?1:-1,a=m[f.column],l=g[f.column];if(a===l)continue;if(a==null)return 1;if(l==null)return-1;if(typeof a=="number"&&typeof l=="number")return a>l?h:-h;const u=String(a).localeCompare(String(l));if(u!==0)return u*h}return 0});const E=typeof e.limit=="number"?w.slice(0,e.limit):w,N=o!=="full",C=[];return N&&C.push("Aggregated on sampled rows. Retry with allowFullScan for full coverage."),{schema:d,rows:E,provenance:{datasetId:e.datasetId,sampled:N,mode:o,processedRows:r.length,totalRows:t,queryHash:v(JSON.stringify({by:s,metrics:e.metrics,filter:e.filter,orderBy:e.orderBy,limit:e.limit,mode:o})),filterCount:c,warnings:C}}},B=async r=>{const e=await S(),t=await e.readColumnStoreRecord(r.datasetId);if(!t)throw new Error("No cached metadata for aggregation.");const o=r.mode??"sample",s=r.sampleSize??R;try{const n=o==="full"?await e.readAllRows(r.datasetId):await e.readSampledRows(r.datasetId,s);if(n.length===0)throw new Error("No rows available for aggregation.");return k(n,r,t.rowCount)}catch(n){if(n instanceof Error&&n.message==="FULL_SCAN_BLOCKED")throw Object.assign(new Error("Full scan requires confirmation."),{code:"FULL_SCAN_BLOCKED"});if(n instanceof Error&&n.message==="AGG_TIMEOUT"&&o==="full"){const c=await e.readSampledRows(r.datasetId,s),i=k(c,{...r,mode:"sample"},t.rowCount);return i.provenance.warnings.push("Full scan timed out. Showing sampled results."),i}throw n}},$=async r=>{const{datasetId:e,n:t=M,withColumns:o=!1}=r;if(!e)throw new Error("datasetId is required for sampling.");const s=await S(),[n,c]=await Promise.all([s.readSampledRows(e,t),o?s.readColumnStoreRecord(e):Promise.resolve(null)]);if(n.length===0)throw new Error("No rows available to sample.");return{rows:n,sampled:!0,columns:c==null?void 0:c.columns}},U=async r=>{const e=performance.now();try{let t;switch(r.action){case"profile":t=await T(r.payload);break;case"sample":t=await $(r.payload);break;case"aggregate":t=await B(r.payload);break;default:throw new Error(`Unknown action: ${r.action}`)}return{id:r.id,ok:!0,result:t,durationMs:performance.now()-e}}catch(t){const o=t instanceof Error?t.message:"Unknown worker error";let s;return t instanceof Error&&t.code==="FULL_SCAN_BLOCKED"?s="Please confirm full scan with the user, then retry with allowFullScan=true.":o.includes("IndexedDB")?s="Browser blocked IndexedDB in this worker; falling back to main thread.":s="Retry with a smaller sample or adjust your query.",{id:r.id,ok:!1,reason:o,hint:s,durationMs:performance.now()-e}}};self.addEventListener("message",r=>{const e=r.data;!e||typeof e.id!="string"||U(e).then(t=>{self.postMessage(t)})});
